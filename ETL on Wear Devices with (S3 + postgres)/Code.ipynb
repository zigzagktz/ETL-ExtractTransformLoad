{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport psycopg2\nimport boto3\nimport sqlalchemy\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import select\nfrom sqlalchemy import inspect\nfrom sqlalchemy import MetaData\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["class process:\n  def __init__( self, dataframe,columns_names):\n    self.dataframe = dataframe\n    self.columns_names = columns_names\n    \n  def change_name(self,dataframe):\n    dataframe.columns = column_names\n    return dataframe\n    \n  def convert(self,dataframe):\n    spark_df= spark.createDataFrame(dataframe)\n    return spark_df\n  \n  def fillna(self, spark_df):\n    spark_df = spark_df.fillna(\"empty\")\n    return spark_df\n    \n    \nclass with_sql:\n  def __init__(self, spark_df):\n    self.spark_df = spark_df\n    \n  def create_view(self, spark_df):\n    spark_df.createOrReplaceTempView(\"sql_df\")\n    pass\n  \n  def show_df(self):\n    return display(sqlContext.sql(\"select * from sql_df\"))\n     \n  def subsetting():\n    spark.sql(\"DROP TABLE IF EXISTS DATA\")\n    spark.sql(\"CREATE TABLE DATA AS (SELECT Name, price, BodyLocation, category, companyname, CompanyCity, CompanyState FROM sql_df) \")\n\nclass exploration:\n  def __init__(self):\n    pass\n  \n  def explore():\n    spark.sql(\"SELECT count(distinct(name)) as unique_device_names from data \").show()\n    spark.sql(\"SELECT count(distinct(companyname)) as unique_company_count from data \").show()\n    spark.sql(\"SELECT count(distinct(companycity)) as unique_city_count from data \").show()\n    spark.sql(\"SELECT count(distinct(companystate)) as unique_state_count from data \").show()\n    spark.sql(\"SELECT distinct(category) as unique_category from data \").show()\n    spark.sql(\"SELECT distinct(bodylocation) as uniquebodylocations from data \").show()\n\n    \nclass connect_database:\n  def __init__(self):\n    engine = create_engine('postgresql://iamtheuser:password@database-1.cdqvm7ggjees.us-east-1.rds.amazonaws.com:5432/postgres')\n    pass\n\n    \nclass load_to_database():\n  def __init__(self):\n    pass\n  \n  def loading():\n    device_name = spark.sql(\"SELECT NAME FROM DATA\").toPandas()\n    device_name.to_sql(\"product_names\",con=engine)\n\n    body_parts = spark.sql(\"SELECT  distinct(BodyLocation) from DATA\").toPandas()\n    body_parts.to_sql(\"body_parts\",con=engine)\n\n    company_info = spark.sql(\"SELECT  COMPANYNAME, COMPANYCITY, COMPANYSTATE from DATA\").toPandas()\n    company_info.to_sql(\"Company_info\",con=engine)\n\n    category = spark.sql(\"SELECT  distinct(Category) from DATA\").toPandas()\n    category.to_sql(\"category\",con=engine)\n\n    sql_df = spark.sql(\"SELECT  * from DATA\").toPandas()\n    sql_df.to_sql(\"spark_dataframe\",con=engine)\n  \n    return [device_name,body_parts,company_info,category.sql_df]\n\nclass facts:\n  def __init__(self):\n    pass\n  def facts():\n    columns= [\"IDs\", \"Product_ID\",\"Category_ID\",\"BodyPart_ID\",\"Company_ID\", \"Price\"]\n    fact_table = pd.DataFrame(engine.execute(\"SELECT * FROM FACTS\"), columns = columns)\n    return fact_table\n  \n\nclass laod_to_s3:\n  def __init__(self):\n    region = \"us-east-1\"\n    aws_access_key_id = \"--put values here--\"\n    aws_secret_access_key = \"--put values here--\"\n\n    s3 = boto3.client(\"s3\", region_name = region,aws_access_key_id= aws_access_key_id, aws_secret_access_key= aws_secret_access_key )\n    s3.create_bucket(Bucket=\"etlwithspark\")\n    s3.put_object( )"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["column_names = [\"Name\",\"Price\",\"BodyLocation\",\"Category\",\"CompanyName\",\"CompanyURL\",\"CompanyMappingLocation\",\"CompanyCity\",\"CompanyState\",\"CompanyCountry\",\"Source\",\"Link\",\"Duplicates\",\"ID\",\"Image\"]\n\ndf = pd.read_csv(\"https://query.data.world/s/oql7pm5zsev3hovxz6svl2yszfmx54\", encoding = \"ISO-8859-1\")\n\np = process(df, column_names) # object creation\ndf =  p.change_name(df)\nsdf = p.convert(df)\nsdf = p.fillna(sdf)\n\nws = with_sql(sdf)   # object creation\nws.create_view(sdf)\nws.show_df()\nws.subsetting()\n\nexp =  exploration()   # object creation\nexp.explore()\n\ncd =  connect_database() # object creation\n\nld =  load_to_database()   # object creation\ndataframe_list = ld.loading()\n\n\nft = facts()  # object creation\nfact_table = ft.facts()\ndataframe_list.append(fact_table)\n\ns3 =  load_to_s3()\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["spark_df.createOrReplaceTempView(\"sql_df\")\ndisplay(sqlContext.sql(\"select * from sql_df\"))\nspark.sql(\"DROP TABLE IF EXISTS DATA\")\nspark.sql(\"CREATE TABLE DATA AS (SELECT Name, price, BodyLocation, category, companyname, CompanyCity, CompanyState FROM sql_df) \")\n\ndef explore():\n  spark.sql(\"SELECT count(distinct(name)) as unique_device_names from data \").show()\n  spark.sql(\"SELECT count(distinct(companyname)) as unique_company_count from data \").show()\n  spark.sql(\"SELECT count(distinct(companycity)) as unique_city_count from data \").show()\n  spark.sql(\"SELECT count(distinct(companystate)) as unique_state_count from data \").show()\n  spark.sql(\"SELECT distinct(category) as unique_category from data \").show()\n  spark.sql(\"SELECT distinct(bodylocation) as uniquebodylocations from data \").show()\n\nengine = create_engine('postgresql://iamtheuser:password@database-1.cdqvm7ggjees.us-east-1.rds.amazonaws.com:5432/postgres')\n\ndef subsetting():\n  device_name = spark.sql(\"SELECT NAME FROM sql_df\").toPandas()\n  device_name.to_sql(\"product_names\",con=engine)\n\n  body_parts = spark.sql(\"SELECT  distinct(BodyLocation) from sql_df\").toPandas()\n  body_parts.to_sql(\"body_parts\",con=engine)\n\n  company_info = spark.sql(\"SELECT  COMPANYNAME, COMPANYCITY, COMPANYSTATE from sql_df\").toPandas()\n  company_info.to_sql(\"Company_info\",con=engine)\n\n  category = spark.sql(\"SELECT  distinct(Category) from sql_df\").toPandas()\n  category.to_sql(\"category\",con=engine)\n\n  sql_df = spark.sql(\"SELECT  * from sql_df\").toPandas()\n  sql_df.to_sql(\"spark_dataframe\",con=engine)\n  \n  return []\n\n\nfact_table = pd.DataFrame(engine.execute(\"SELECT * FROM FACTS\"), columns =[\"IDs\", \"Product_ID\",\"Category_ID\",\"BodyPart_ID\",\"Company_ID\", \"Price\"])\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["region = \"us-east-1\"\naws_access_key_id = \"__put values here__\"\naws_secret_access_key = \"__put values here__\"\n\ns3 = boto3.client(\"s3\", region_name = region,aws_access_key_id= aws_access_key_id, aws_secret_access_key= aws_secret_access_key )\ns3.put_object(Key = , Body= )"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["\ndf.columns\ncolumn_names = [\"Name\",\"Price\",\"BodyLocation\",\"Category\",\"CompanyName\",\"CompanyURL\",\"CompanyMappingLocation\",\"CompanyCity\",\"CompanyState\",\"CompanyCountry\",\"Source\",\"Link\",\"Duplicates\",\"ID\",\"Image\"]"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["spark_df= spark.createDataFrame(df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["spark_df.columns"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["print(\"Total number of rows {}\".format(spark_df.count()))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["spark_df = spark_df.fillna(\"empty\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["spark_df.createOrReplaceTempView(\"sql_df\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(sqlContext.sql(\"select * from sql_df\"))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["The dimension are\n1. Body Location\n2. Category\n3. Company Name, company city, company state, company country"],"metadata":{}},{"cell_type":"code","source":["# Extract important columns \nspark.sql(\"DROP TABLE IF EXISTS DATA\")\nspark.sql(\"CREATE TABLE DATA AS (SELECT Name, price, BodyLocation, category, companyname, CompanyCity, CompanyState FROM sql_df) \")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["spark.sql(\"SELECT count(distinct(name)) as unique_device_names from data \").show()\nspark.sql(\"SELECT count(distinct(companyname)) as unique_company_count from data \").show()\nspark.sql(\"SELECT count(distinct(companycity)) as unique_city_count from data \").show()\nspark.sql(\"SELECT count(distinct(companystate)) as unique_state_count from data \").show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["spark.sql(\"SELECT distinct(category) as unique_category from data \").show()\nspark.sql(\"SELECT distinct(bodylocation) as uniquebodylocations from data \").show()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["I have created the dimension in the database using pgadmin. I now have to populate them here."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["\ndevice_name = spark.sql(\"SELECT NAME FROM sql_df\").toPandas()\ndevice_name.to_sql(\"product_names\",con=engine)\ndisplay(device_name)\n\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["body_parts = spark.sql(\"SELECT  distinct(BodyLocation) from sql_df\").toPandas()\nbody_parts.to_sql(\"body_parts\",con=engine)\ndisplay(body_parts)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["body_parts = spark.sql(\"SELECT  COMPANYNAME, COMPANYCITY, COMPANYSTATE from sql_df\").toPandas()\nbody_parts.to_sql(\"Company_info\",con=engine)\ndisplay(body_parts)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["category = spark.sql(\"SELECT  distinct(Category) from sql_df\").toPandas()\ncategory.to_sql(\"category\",con=engine)\ndisplay(category, display_id=\"cat\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["sql_df = spark.sql(\"SELECT  * from sql_df\").toPandas()\nsql_df.to_sql(\"spark_dataframe\",con=engine)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["fact_table = pd.DataFrame(engine.execute(\"SELECT * FROM FACTS\"), columns =[\"IDs\", \"Product_ID\",\"Category_ID\",\"BodyPart_ID\",\"Company_ID\", \"Price\"])\nfact_table = spark.createDataFrame(fact_table)\ndisplay(fact_table)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["\nregion = \"us-east-1\"\naws_access_key_id = \"--put values here--\"\naws_secret_access_key = \"--put values here--\"\n\ns3 = boto3.client(\"s3\", region_name = region,aws_access_key_id= aws_access_key_id, aws_secret_access_key= aws_secret_access_key )\ns3.create_bucket(Bucket=\"etlwithspark\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%fs\nls /FileStore/tables/sales.csv"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["trial_df = pd.DataFrame( {\"a\":[2,3,4], \"b\":[4,6,4] }  )\ntrial_df\ntrial_df = pickle.dumps(trial_df)\ns3.put_object(Body = trial_df, Bucket = 'etlwithspark',Key= 'filename.txt')\npd.to_pickle(trial_df,\"s3://etlwithspark\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["s3.get_object(\"filename.txt\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["pd.read_json(s3.download_file(\"etlwithspark\",\"filename.txt\",\"downloaded\"))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["print(open('downloaded',encoding=\"utf8\").read())"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["s3.download_file('etlwithpython', 'filename.txt', 'trial_dataframe')"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["open('hello.txt').write('Hello, world!')\n\n# Upload the file to S3\n#s3_client.upload_file('hello.txt', 'MyBucket', 'hello-remote.txt')\n\n# Download the file from S3\n#s3_client.download_file('MyBucket', 'hello-remote.txt', 'hello2.txt')\n#print(open('hello2.txt').read())"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["import pickle\ndf = pickle.dumps(fact_table)\ns3.put_object(Body = df, Bucket = 'etlwithspark',Key= 'my/key/including/filename.txt')"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["import s3fs\ns3_url = 's3://etlwithspark/bucket.parquet.gzip'\nfact_table.to_parquet(s3_url, compression='gzip')"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["import s3fs\nfs = s3fs.S3FileSystem(anon=False, key='<Access Key>', secret='<Secret Key>')"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["import pickle\ndf = pickle.dumps(fact_table)\n\nfrom io import BytesIO\nbytesIO = BytesIO()\nbytesIO.write(df)\nbytesIO.seek(0)\n\n\n.set_contents_from_file(bytesIO)\n"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["pd.DataFrame(fact_table,columns =[\"IDs\", \"Product_ID\",\"Category_ID\",\"BodyPart_ID\",\"Company_ID\", \"Price\"] ) "],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["AWS_BUCKET_NAME = \"sparkpostgres\"\nMOUNT_NAME = \"mymount\"\ndbutils.fs.mount(\"s3a://%s\" % AWS_BUCKET_NAME, \"/mnt/%s\" % MOUNT_NAME)\ndisplay(dbutils.fs.ls(\"/mnt/%s\" % MOUNT_NAME))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["meta = MetaData(engine)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["inspector = inspect(engine)\n# Get table information\nprint(inspector.get_table_names())"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["print(meta.reflect(engine))"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# Code to save file to directory and deleting it with recursion so that first the folder gets empty and than get deleted\ntry_file = sqlContext.sql(\"select * from sql_df\")\ntry_file.write.parquet(\"try_file.paruqet\")\ndbutils.fs.rm(\"/try_file.paruqet/\", True)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["import boto3\ns3 =  boto3.client(\"s3\", region_name = region,aws_access_key_id= aws_access_key_id, aws_secret_access_key= aws_secret_access_key )\nobj = s3.get_object(Bucket='etlwithspark', Key= 'filename.txt')\n\n\nimport codecs\nbody = obj['Body']\n\nfor ln in codecs.getreader('utf-8')(body):\n\tprocess(ln)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["\n\nobj =s3.get_object(Bucket='etlwithspark', Key='medals.csv')\ngrid_sizes = pd.read_csv(obj['Body'])"],"metadata":{},"outputs":[],"execution_count":50}],"metadata":{"name":"small ETL","notebookId":1573248800639678},"nbformat":4,"nbformat_minor":0}
